# Topic Modelling using Tfidf + Non-negative Matrix Factorization (NMF)
# Tfidf takes into account word frequency while NMF only factorizes data into matrices
# code citation Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF

# set up tfidf using TfidfVectorizer
word_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=800, stop_words='english')
# fit tfidf to the vocab
tfidf = word_vectorizer.fit_transform('vocab')

# Non-Negative Matrix Factorization with Tfidf
nmf = NMF(n_components=50, random_state=0, alpha=.1, l1_ratio=.5, init='nndsvd', shuffle=True).fit(tfidf)
# code citation https://radimrehurek.com/gensim/index.html

# Topic Modelling using Gensim 
import gensim
from gensim import corpora
# Split words
string = [string.split()]
# Create dictionary
dictionary = corpora.Dictionary(string)
word_matrix = [dictionary.doc2bow(doc) for doc in string]
# Create the model
Lda = gensim.models.ldamodel.LdaModel
ldamodel = Lda(word_matrix, num_topics=5, id2word = dictionary, passes=50)
#passes equal epochs - how many times the models keeps training
# Show topic
topics = ldamodel.show_topics(num_topics=1, num_words=10, formatted=False)
topics[0][1] # show the word and the possibility 
